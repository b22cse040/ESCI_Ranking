{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CgMdP-0ohKh"
      },
      "outputs": [],
      "source": [
        "from re import escape\n",
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "processed_train_df = pd.read_csv(\n",
        "    'processed_train_updated.csv',\n",
        "    quoting=csv.QUOTE_ALL,\n",
        "    escapechar='\\\\',\n",
        "    engine='python',\n",
        "    encoding='utf-8-sig',\n",
        "    on_bad_lines = 'skip'\n",
        ")\n",
        "\n",
        "processed_test_df = pd.read_csv(\n",
        "    'processed_test_updated.csv',\n",
        "    quoting=csv.QUOTE_ALL,\n",
        "    escapechar='\\\\',\n",
        "    engine='python',\n",
        "    encoding='utf-8-sig'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(processed_train_df.info())\n",
        "print('\\n')\n",
        "print('=' * 65)\n",
        "print('\\n')\n",
        "processed_test_df.info()\n",
        "print('\\n')\n",
        "print('=' * 65)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypkzxmKqpae_",
        "outputId": "94e91cf9-4f8c-422c-a12e-7dc6292402a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 50000 entries, 0 to 49999\n",
            "Data columns (total 3 columns):\n",
            " #   Column         Non-Null Count  Dtype  \n",
            "---  ------         --------------  -----  \n",
            " 0   query          50000 non-null  object \n",
            " 1   product_input  50000 non-null  object \n",
            " 2   esci_label     50000 non-null  float64\n",
            "dtypes: float64(1), object(2)\n",
            "memory usage: 1.1+ MB\n",
            "None\n",
            "\n",
            "\n",
            "=================================================================\n",
            "\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10000 entries, 0 to 9999\n",
            "Data columns (total 3 columns):\n",
            " #   Column         Non-Null Count  Dtype  \n",
            "---  ------         --------------  -----  \n",
            " 0   query          10000 non-null  object \n",
            " 1   product_input  10000 non-null  object \n",
            " 2   esci_label     10000 non-null  float64\n",
            "dtypes: float64(1), object(2)\n",
            "memory usage: 234.5+ KB\n",
            "\n",
            "\n",
            "=================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Bi-Encoder"
      ],
      "metadata": {
        "id": "FA3dWU24pghZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "model_name = \"microsoft/deberta-v3-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "encoder = AutoModel.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYqEzF8NpeKE",
        "outputId": "e2e79652-8189-4044-faf5-7e9a7346f7da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Creating dict for product info as prod_groups\n",
        "# and esci_label as label_groups\n",
        "prod_groups_train = defaultdict(list)\n",
        "prod_groups_test  = defaultdict(list)\n",
        "label_groups_train = defaultdict(list)\n",
        "label_groups_test  = defaultdict(list)\n",
        "\n",
        "def get_dicts(df, prod_groups, label_groups):\n",
        "    for _, row in df.iterrows():\n",
        "        query = row[\"query\"]\n",
        "        product = row[\"product_input\"]\n",
        "        relevance = float(row[\"esci_label\"])\n",
        "\n",
        "        prod_groups[query].append(product)\n",
        "        label_groups[query].append(relevance)\n",
        "\n",
        "get_dicts(processed_train_df, prod_groups_train, label_groups_train)\n",
        "get_dicts(processed_test_df, prod_groups_test, label_groups_test)"
      ],
      "metadata": {
        "id": "9LiqAaM0pkg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import random\n",
        "\n",
        "class ESCI_Dataset(Dataset):\n",
        "    def __init__(self, tokenizer, prod_groups, label_groups, max_len=128):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.pairs = []\n",
        "        self.labels = []\n",
        "\n",
        "        ## Labels are 0.0(I), 0.01(C), 0.1(S) and 1.0(E),\n",
        "        ## Models would prefer to promote with labels 1.0 and 0.1\n",
        "        ## over 0.01 and 0.0\n",
        "        for query in prod_groups:\n",
        "            product_info = prod_groups[query]\n",
        "            labels = label_groups[query]\n",
        "\n",
        "            E_indices = [i for i, l in enumerate(labels) if l > 0.1]\n",
        "            S_indices = [i for i, l in enumerate(labels) if l > 0.01]\n",
        "            C_indices = [i for i, l in enumerate(labels) if l > 0]\n",
        "            I_indices = [i for i, l in enumerate(labels) if l == 0]\n",
        "\n",
        "            for idx in E_indices:\n",
        "                pos_product = product_info[idx]\n",
        "                self.pairs.append((query, pos_product))\n",
        "                self.labels.append(1.0)\n",
        "\n",
        "            for idx in S_indices:\n",
        "                pos_product = product_info[idx]\n",
        "                self.pairs.append((query, pos_product))\n",
        "                self.labels.append(0.1)\n",
        "\n",
        "            for idx in C_indices:\n",
        "                pos_product = product_info[idx]\n",
        "                self.pairs.append((query, pos_product))\n",
        "                self.labels.append(0.01)\n",
        "\n",
        "            for idx in I_indices:\n",
        "                pos_product = product_info[idx]\n",
        "                self.pairs.append((query, pos_product))\n",
        "                self.labels.append(0.0)\n",
        "\n",
        "\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        query, product = self.pairs[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        query_encoded = self.tokenizer(\n",
        "            query,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        product_encoded = self.tokenizer(\n",
        "            product,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"query_input_ids\": query_encoded[\"input_ids\"].squeeze(0),\n",
        "            \"query_attention_mask\": query_encoded[\"attention_mask\"].squeeze(0),\n",
        "            \"product_input_ids\": product_encoded[\"input_ids\"].squeeze(0),\n",
        "            \"product_attention_mask\": product_encoded[\"attention_mask\"].squeeze(0),\n",
        "            \"label\": torch.tensor(label, dtype=torch.float)\n",
        "        }"
      ],
      "metadata": {
        "id": "tjy-3-HYpmkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "train_dataset = ESCI_Dataset(tokenizer, prod_groups_train, label_groups_train, max_len=128)\n",
        "test_dataset = ESCI_Dataset(tokenizer, prod_groups_test, label_groups_test, max_len=128)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
      ],
      "metadata": {
        "id": "M64atxs9pnwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class BiEncoder(nn.Module):\n",
        "    def __init__(self, model_name, dropout_rate=0.1):\n",
        "        super(BiEncoder, self).__init__()\n",
        "        self.encoder = AutoModel.from_pretrained(model_name)\n",
        "        hidden_size = self.encoder.config.hidden_size\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.scorer = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def encode(self, input_ids, attention_mask):\n",
        "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.last_hidden_state[:, 0]  # CLS token\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        return pooled_output\n",
        "\n",
        "    def forward(self, query_input_ids, query_attention_mask, product_input_ids, product_attention_masks):\n",
        "        query_vec = self.encode(query_input_ids, query_attention_mask)\n",
        "        product_vec = self.encode(product_input_ids, product_attention_masks)\n",
        "\n",
        "        ## Cosine Similarity\n",
        "        logits = F.cosine_similarity(query_vec, product_vec, dim=1)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "IHfvwE6mprTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Loss function as RCR Loss with Softplus function\n",
        "\n",
        "def list_ce_loss(logits, labels):\n",
        "  \"\"\"\n",
        "  logits: torch.Tensor,\n",
        "  labels: torch.Tensor\n",
        "  \"\"\"\n",
        "  true_dist = F.softmax(labels, dim = 0)\n",
        "  log_pred_dist = F.log_softmax(logits, dim = 0)\n",
        "  return -torch.sum(true_dist * log_pred_dist)\n",
        "\n",
        "def rcr_loss_function(logits, labels, alpha):\n",
        "  \"\"\"\n",
        "  logits: torch.Tensor,\n",
        "  labels: torch.Tensor\n",
        "  \"\"\"\n",
        "  reg_preds = F.softplus(logits)\n",
        "  reg_loss = F.mse_loss(reg_preds, labels)\n",
        "  listwise_loss = list_ce_loss(logits, labels)\n",
        "  return (1 - alpha) * reg_loss + alpha * listwise_loss"
      ],
      "metadata": {
        "id": "u-IH2StBptHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "from transformers import get_scheduler\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BiEncoder(model_name).to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=8e-6, weight_decay=0.01)\n",
        "alpha = 0.5\n",
        "num_epochs = 3\n",
        "\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=\"linear\",\n",
        "    optimizer = optimizer,\n",
        "    num_warmup_steps = 100,\n",
        "    num_training_steps = num_epochs * len(train_loader)\n",
        ")\n",
        "\n",
        "global_step = 0.0\n",
        "losses = []\n",
        "\n",
        "model.train()\n",
        "for epoch in range(1):\n",
        "    loop = tqdm(train_loader, desc = f\"Epoch {epoch + 1} / {num_epochs}\")\n",
        "\n",
        "    for batch in loop:\n",
        "        query_input_ids = batch[\"query_input_ids\"].to(device)\n",
        "        query_attention_mask = batch[\"query_attention_mask\"].to(device)\n",
        "\n",
        "        product_input_ids = batch[\"product_input_ids\"].to(device)\n",
        "        product_attention_mask = batch[\"product_attention_mask\"].to(device)\n",
        "\n",
        "        labels = batch[\"label\"].to(device)\n",
        "\n",
        "        logits = model(\n",
        "            query_input_ids,\n",
        "            query_attention_mask,\n",
        "            product_input_ids,\n",
        "            product_attention_mask\n",
        "        )\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss = rcr_loss_function(logits, labels, alpha)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        global_step += 1\n",
        "\n",
        "        loop.set_postfix(loss = loss.item())\n",
        "\n",
        "        losses.append(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJCZuZoPpvw5",
        "outputId": "2ff712fa-4489-4dab-bb1d-a069c5176f9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 / 3: 100%|██████████| 13894/13894 [1:59:55<00:00,  1.93it/s, loss=0.736]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ndcg_score\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "model.eval()\n",
        "query_to_scores = defaultdict(list)\n",
        "query_to_labels = defaultdict(list)\n",
        "\n",
        "test_pairs = test_dataset.pairs\n",
        "test_labels = test_dataset.labels\n",
        "\n",
        "batch_size = 8\n",
        "with torch.no_grad():\n",
        "    for i in tqdm(range(0, len(test_pairs), batch_size), desc=\"Evaluating\"):\n",
        "        batch_pairs = test_pairs[i:i+batch_size]\n",
        "        batch_labels = test_labels[i:i+batch_size]\n",
        "\n",
        "        queries = [q for q, _ in batch_pairs]\n",
        "        products = [p for _, p in batch_pairs]\n",
        "\n",
        "        query_enc = tokenizer(\n",
        "            queries,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=128,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        product_enc = tokenizer(\n",
        "            products,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=128,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        query_input_ids = query_enc[\"input_ids\"].to(device)\n",
        "        query_attention_mask = query_enc[\"attention_mask\"].to(device)\n",
        "        product_input_ids = product_enc[\"input_ids\"].to(device)\n",
        "        product_attention_mask = product_enc[\"attention_mask\"].to(device)\n",
        "\n",
        "        scores = model(\n",
        "            query_input_ids=query_input_ids,\n",
        "            query_attention_mask=query_attention_mask,\n",
        "            product_input_ids=product_input_ids,\n",
        "            product_attention_masks=product_attention_mask\n",
        "        ).cpu().tolist()\n",
        "\n",
        "        for q, s, l in zip(queries, scores, batch_labels[i:i+batch_size]):\n",
        "            query_to_scores[q].append(s)\n",
        "            query_to_labels[q].append(l)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAEwCEbkE1rB",
        "outputId": "b6ae807d-8f8e-4444-9ef3-ee1f5ffc9234"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 2671/2671 [06:58<00:00,  6.38it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ndcg_total = 0\n",
        "count = 0\n",
        "\n",
        "for q in query_to_labels:\n",
        "    if sum(query_to_labels[q]) > 0:\n",
        "        y_true = [query_to_labels[q]]\n",
        "        y_score = [query_to_scores[q]]\n",
        "        try:\n",
        "            ndcg = ndcg_score(y_true, y_score, k=10)\n",
        "            ndcg_total += ndcg\n",
        "            count += 1\n",
        "        except:\n",
        "            pass  # In case of malformed input, skip\n",
        "\n",
        "avg_ndcg_10 = ndcg_total / count if count > 0 else 0\n",
        "print(f\"Average NDCG@10: {avg_ndcg_10:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ahC7VZVTrbe",
        "outputId": "c53255e8-0ab8-419d-91ee-a39913208b78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average NDCG@10: 0.7140\n"
          ]
        }
      ]
    }
  ]
}