{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sLXMFjiyyQO2"
      },
      "outputs": [],
      "source": [
        "from re import escape\n",
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "processed_train_df = pd.read_csv('processed_train_updated.csv')\n",
        "processed_test_df = pd.read_csv('processed_test_updated.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(processed_train_df.info())\n",
        "print('\\n')\n",
        "print('=' * 65)\n",
        "print('\\n')\n",
        "processed_test_df.info()\n",
        "print('\\n')\n",
        "print('=' * 65)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMruPEy8zIx6",
        "outputId": "4c6915ac-c6b2-4d49-d205-93fbdf0155dd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 50000 entries, 0 to 49999\n",
            "Data columns (total 3 columns):\n",
            " #   Column         Non-Null Count  Dtype  \n",
            "---  ------         --------------  -----  \n",
            " 0   query          50000 non-null  object \n",
            " 1   product_input  50000 non-null  object \n",
            " 2   esci_label     50000 non-null  float64\n",
            "dtypes: float64(1), object(2)\n",
            "memory usage: 1.1+ MB\n",
            "None\n",
            "\n",
            "\n",
            "=================================================================\n",
            "\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10000 entries, 0 to 9999\n",
            "Data columns (total 3 columns):\n",
            " #   Column         Non-Null Count  Dtype  \n",
            "---  ------         --------------  -----  \n",
            " 0   query          10000 non-null  object \n",
            " 1   product_input  10000 non-null  object \n",
            " 2   esci_label     10000 non-null  float64\n",
            "dtypes: float64(1), object(2)\n",
            "memory usage: 234.5+ KB\n",
            "\n",
            "\n",
            "=================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_counts = processed_train_df[\"query\"].value_counts()\n",
        "qualifiable_queries = (query_counts > 1).sum()\n",
        "qualifiable_queries"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63cijmVuzLWl",
        "outputId": "82846317-30b8-4555-dae4-8956fff524c8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.int64(7348)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "model_name = \"microsoft/deberta-v3-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "encoder = AutoModel.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wq0O7JiKzNWF",
        "outputId": "d6e85386-2478-43ba-b2aa-f84ba4402860"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Creating dict for product info as prod_groups\n",
        "# and esci_label as label_groups\n",
        "prod_groups_train = defaultdict(list)\n",
        "prod_groups_test  = defaultdict(list)\n",
        "label_groups_train = defaultdict(list)\n",
        "label_groups_test  = defaultdict(list)\n",
        "\n",
        "def get_dicts(df, prod_groups, label_groups):\n",
        "  for _, row in df.iterrows():\n",
        "    query = row[\"query\"]\n",
        "    product = row[\"product_input\"]\n",
        "    relevance = float(row[\"esci_label\"])\n",
        "\n",
        "    prod_groups[query].append(product)\n",
        "    label_groups[query].append(relevance)\n",
        "\n",
        "get_dicts(processed_train_df, prod_groups_train, label_groups_train)\n",
        "get_dicts(processed_test_df, prod_groups_test, label_groups_test)"
      ],
      "metadata": {
        "id": "7PUJzeDSzRE1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class ESCIDataset(Dataset):\n",
        "    def __init__(self, tokenizer, prod_groups, label_groups, max_len=128):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.pairs = []\n",
        "        self.reg_labels = []\n",
        "        self.cls_labels = []\n",
        "\n",
        "        ## Labels are 0.0(I), 0.01(C), 0.1(S) and 1.0(E),\n",
        "        ## Models would prefer to promote with labels 1.0 and 0.1\n",
        "        ## over 0.01 and 0.0\n",
        "\n",
        "        score_to_index = {0.0: 0, 0.01: 1, 0.1: 2, 1.0: 3}\n",
        "\n",
        "        for query in prod_groups:\n",
        "            product_info = prod_groups[query]\n",
        "            labels = label_groups[query]\n",
        "\n",
        "            for idx, label in enumerate(labels):\n",
        "                self.pairs.append((query, product_info[idx]))\n",
        "                self.reg_labels.append(label)\n",
        "                self.cls_labels.append(score_to_index[label])\n",
        "\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        query, product = self.pairs[idx]\n",
        "        reg_label = self.reg_labels[idx]\n",
        "        cls_label = self.cls_labels[idx]\n",
        "\n",
        "        encoded = self.tokenizer(\n",
        "            query,\n",
        "            product,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        encoded = {k: v.squeeze(0) for k, v in encoded.items()}\n",
        "        encoded[\"reg_label\"] = torch.tensor(reg_label, dtype=torch.float)\n",
        "        encoded[\"cls_label\"] = torch.tensor(cls_label, dtype=torch.long)\n",
        "\n",
        "        return encoded"
      ],
      "metadata": {
        "id": "4a7MUKoAzYwU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "class CrossEncoder(nn.Module):\n",
        "    def __init__(self, encoder):\n",
        "        super(CrossEncoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        hidden_size = encoder.config.hidden_size\n",
        "        self.reg_head = nn.Linear(hidden_size, 1)   # for regression\n",
        "        self.cls_head = nn.Linear(hidden_size, 4)   # for classification (4 classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
        "        reg_logits = self.reg_head(pooled_output).squeeze(-1)\n",
        "        cls_logits = self.cls_head(pooled_output)\n",
        "        return reg_logits, cls_logits"
      ],
      "metadata": {
        "id": "npcpWtMvzXZv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def list_ce_loss(logits, labels):\n",
        "    true_dist = F.softmax(labels, dim=0)\n",
        "    log_pred_dist = F.log_softmax(logits, dim=0)\n",
        "    return -torch.sum(true_dist * log_pred_dist)\n",
        "\n",
        "def rcr_loss_function(logits, reg_labels, alpha=0.3):\n",
        "    reg_preds = F.softplus(logits)\n",
        "    reg_loss = F.mse_loss(reg_preds, reg_labels)\n",
        "    listwise_loss = list_ce_loss(logits, reg_labels)\n",
        "    return (1 - alpha) * reg_loss + alpha * listwise_loss\n",
        "\n",
        "def multitask_loss(reg_logits, cls_logits, reg_labels, cls_labels, x=1/3, alpha=0.5):\n",
        "    \"\"\"\n",
        "    x: weight for classification vs regression\n",
        "    alpha: weight inside RCR loss\n",
        "    \"\"\"\n",
        "    rcr = rcr_loss_function(reg_logits, reg_labels, alpha)\n",
        "    ce = F.cross_entropy(cls_logits, cls_labels)\n",
        "    return (1 - x) * rcr + x * ce"
      ],
      "metadata": {
        "id": "uYG-vgxn1z73"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "train_dataset = ESCIDataset(tokenizer, prod_groups_train, label_groups_train, max_len=128)\n",
        "test_dataset = ESCIDataset(tokenizer, prod_groups_test, label_groups_test, max_len=128)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "id": "I0YHAPu52fHi"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "from transformers import get_scheduler\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CrossEncoder(encoder).to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=8e-6, weight_decay=0.01)\n",
        "num_epochs = 1\n",
        "alpha = 0.5    # Inner RCR: (1-alpha)*MSE + alpha*ListCE\n",
        "x = 0.33       # Outer MTL: (1-x)*RCR + x*CrossEntropy\n",
        "\n",
        "num_training_steps = num_epochs * len(train_loader)\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=\"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=100,\n",
        "    num_training_steps=num_training_steps,\n",
        ")\n",
        "\n",
        "global_step = 0\n",
        "losses = []\n",
        "\n",
        "model.train()\n",
        "progress_bar = tqdm(total=num_epochs * len(train_loader), desc=\"Training\", ncols=100)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        reg_labels = batch[\"reg_label\"].to(device)\n",
        "        cls_labels = batch[\"cls_label\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        reg_logits, cls_logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = multitask_loss(reg_logits, cls_logits, reg_labels, cls_labels, x=x, alpha=alpha)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "        global_step += 1\n",
        "\n",
        "        progress_bar.set_postfix(loss=f\"{loss.item():.4f}\", step=global_step)\n",
        "        progress_bar.update(1)\n",
        "\n",
        "progress_bar.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycjfgl8v2qr4",
        "outputId": "e2cf0e74-dbb8-469d-b197-c612512d731f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|█████████████████████████| 3125/3125 [28:05<00:00,  1.85it/s, loss=1.2160, step=3125]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ndcg_score\n",
        "from collections import defaultdict\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "model.eval()\n",
        "\n",
        "query_to_scores = defaultdict(list)\n",
        "query_to_labels = defaultdict(list)\n",
        "\n",
        "all_cls_preds = []\n",
        "all_cls_trues = []\n",
        "\n",
        "test_pairs = test_dataset.pairs\n",
        "reg_labels = test_dataset.reg_labels\n",
        "cls_labels = test_dataset.cls_labels\n",
        "\n",
        "batch_size = 16\n",
        "with torch.no_grad():\n",
        "    for i in tqdm(range(0, len(test_pairs), batch_size), desc=\"Evaluating\"):\n",
        "        batch_pairs = test_pairs[i:i+batch_size]\n",
        "        batch_reg_labels = reg_labels[i:i+batch_size]\n",
        "        batch_cls_labels = cls_labels[i:i+batch_size]\n",
        "\n",
        "        queries = [q for q, _ in batch_pairs]\n",
        "        products = [p for _, p in batch_pairs]\n",
        "\n",
        "        encoded = tokenizer(\n",
        "            queries,\n",
        "            products,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=128,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        input_ids = encoded[\"input_ids\"].to(device)\n",
        "        attention_mask = encoded[\"attention_mask\"].to(device)\n",
        "\n",
        "        reg_logits, cls_logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        reg_scores = F.softplus(reg_logits).cpu().tolist()\n",
        "        cls_preds = torch.argmax(F.softmax(cls_logits, dim=-1), dim=-1).cpu().tolist()\n",
        "        cls_trues = batch_cls_labels\n",
        "\n",
        "        for q, s, l in zip(queries, reg_scores, batch_reg_labels):\n",
        "            query_to_scores[q].append(s)\n",
        "            query_to_labels[q].append(l)\n",
        "\n",
        "        all_cls_preds.extend(cls_preds)\n",
        "        all_cls_trues.extend(cls_trues)\n",
        "\n",
        "ndcg_total = 0\n",
        "qualifiable_count = 0\n",
        "\n",
        "for q in query_to_labels:\n",
        "    labels = query_to_labels[q]\n",
        "    scores = query_to_scores[q]\n",
        "    if len(labels) > 1 and sum(labels) > 0:\n",
        "        try:\n",
        "            ndcg = ndcg_score([labels], [scores], k=10)\n",
        "            ndcg_total += ndcg\n",
        "            qualifiable_count += 1\n",
        "        except ValueError:\n",
        "            continue\n",
        "\n",
        "avg_ndcg_10 = ndcg_total / qualifiable_count if qualifiable_count > 0 else 0\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy = accuracy_score(all_cls_trues, all_cls_preds)\n",
        "\n",
        "print(f\"Average NDCG@10 (for {qualifiable_count} qualifiable queries): {avg_ndcg_10:.4f}\")\n",
        "print(f\"Classification Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FMqNWU-E8bf",
        "outputId": "696173b5-0ae0-4489-8a8f-f85818a749d8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 625/625 [01:54<00:00,  5.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average NDCG@10 (for 1310 qualifiable queries): 0.9215\n",
            "Classification Accuracy: 0.6864\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "test_df = pd.DataFrame({\n",
        "    \"query\": [q for q, _ in test_dataset.pairs],\n",
        "    \"reg_label\": test_dataset.reg_labels,\n",
        "    \"cls_label\": test_dataset.cls_labels,\n",
        "})\n",
        "\n",
        "grouped = test_df.groupby(\"query\")[\"reg_label\"]\n",
        "multi_item_queries = grouped.count() > 1\n",
        "has_relevant = grouped.sum() > 0\n",
        "qualifiable_queries = multi_item_queries & has_relevant\n",
        "num_qualifiable = qualifiable_queries.sum()\n",
        "\n",
        "print(f\"Number of qualifiable queries for NDCG@10: {num_qualifiable}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EH-PjdpFz1u",
        "outputId": "b61f56d3-25f5-4eb4-f3f2-71ccfa21e70b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of qualifiable queries for NDCG@10: 1310\n"
          ]
        }
      ]
    }
  ]
}